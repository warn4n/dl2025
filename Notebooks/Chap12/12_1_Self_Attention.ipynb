{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/warn4n/dl2025/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook 12.1: Self Attention**\n",
        "\n",
        "This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
        "\n"
      ],
      "metadata": {
        "id": "t9vk9Elugvmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ],
      "metadata": {
        "id": "9OJkkoNqCVK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 3\n",
        "# Number of dimensions of each input\n",
        "D = 4\n",
        "# Create an empty list\n",
        "all_x = []\n",
        "# Create elements x_n and append to list\n",
        "for n in range(N):\n",
        "  all_x.append(np.random.normal(size=(D,1)))\n",
        "# Print out the list\n",
        "print(all_x)\n"
      ],
      "metadata": {
        "id": "oAygJwLiCSri",
        "outputId": "00ce11ca-024f-4292-c51e-12eebeef94df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 1.78862847],\n",
            "       [ 0.43650985],\n",
            "       [ 0.09649747],\n",
            "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
            "       [-0.35475898],\n",
            "       [-0.08274148],\n",
            "       [-0.62700068]]), array([[-0.04381817],\n",
            "       [-0.47721803],\n",
            "       [-1.31386475],\n",
            "       [ 0.88462238]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
      ],
      "metadata": {
        "id": "W2iHFbtKMaDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_q = np.random.normal(size=(D,D))\n",
        "omega_k = np.random.normal(size=(D,D))\n",
        "omega_v = np.random.normal(size=(D,D))\n",
        "beta_q = np.random.normal(size=(D,1))\n",
        "beta_k = np.random.normal(size=(D,1))\n",
        "beta_v = np.random.normal(size=(D,1))"
      ],
      "metadata": {
        "id": "79TSK7oLMobe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the queries, keys, and values for each input"
      ],
      "metadata": {
        "id": "VxaKQtP3Ng6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make three lists to store queries, keys, and values\n",
        "all_queries = []\n",
        "all_keys = []\n",
        "all_values = []\n",
        "# For every input\n",
        "for x in all_x:\n",
        "  # TODO -- compute the keys, queries and values.\n",
        "  # Replace these three lines\n",
        "  query = omega_q @ x + beta_q\n",
        "  key = omega_k @ x + beta_k\n",
        "  value = omega_v @ x + beta_v\n",
        "\n",
        "\n",
        "  all_queries.append(query)\n",
        "  all_keys.append(key)\n",
        "  all_values.append(value)"
      ],
      "metadata": {
        "id": "TwDK2tfdNmw9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"
      ],
      "metadata": {
        "id": "Se7DK6PGPSUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(items_in):\n",
        "\n",
        "  # TODO Compute the elements of items_out\n",
        "  # Replace this line\n",
        "  items_out = np.exp(items_in) / np.sum(np.exp(items_in))\n",
        "\n",
        "  return items_out ;"
      ],
      "metadata": {
        "id": "u93LIcE5PoiM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp([0,1,2,3,4])"
      ],
      "metadata": {
        "id": "motjayolf7e_",
        "outputId": "a3fa8d39-5e9e-4b99-da68-4437c3842707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.        ,  2.71828183,  7.3890561 , 20.08553692, 54.59815003])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute the self attention values:"
      ],
      "metadata": {
        "id": "8aJVhbKDW7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create emptymlist for output\n",
        "all_x_prime = []\n",
        "\n",
        "# For each output\n",
        "for n in range(N):\n",
        "  # Create list for dot products of query N with all keys\n",
        "  all_km_qn = []\n",
        "  # Compute the dot products\n",
        "  for key in all_keys:\n",
        "    # TODO -- compute the appropriate dot product\n",
        "    # Replace this line\n",
        "    dot_product = key.T @ all_queries[n]\n",
        "\n",
        "    # Store dot product\n",
        "    all_km_qn.append(dot_product)\n",
        "\n",
        "  # Compute dot product\n",
        "  attention = softmax(all_km_qn)\n",
        "  # Print result (should be positive sum to one)\n",
        "  print(\"Attentions for output \", n)\n",
        "  print(attention)\n",
        "\n",
        "  # TODO: Compute a weighted sum of all of the values according to the attention\n",
        "  # (equation 12.3)\n",
        "  # Replace this line\n",
        "  x_prime = np.zeros((D,1))\n",
        "  for i in range(N):\n",
        "    x_prime += attention[i] * all_values[i]\n",
        "\n",
        "  all_x_prime.append(x_prime)\n",
        "\n",
        "\n",
        "# Print out true values to check you have it correct\n",
        "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
        "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
        "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
        "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
        "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
        "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n"
      ],
      "metadata": {
        "id": "yimz-5nCW6vQ",
        "outputId": "ffc216f2-17fd-42fc-c581-2c9808f79df0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attentions for output  0\n",
            "[[[1.24326146e-13]]\n",
            "\n",
            " [[9.98281489e-01]]\n",
            "\n",
            " [[1.71851130e-03]]]\n",
            "Attentions for output  1\n",
            "[[[2.79525306e-12]]\n",
            "\n",
            " [[5.85506360e-03]]\n",
            "\n",
            " [[9.94144936e-01]]]\n",
            "Attentions for output  2\n",
            "[[[0.00505708]]\n",
            "\n",
            " [[0.00654776]]\n",
            "\n",
            " [[0.98839516]]]\n",
            "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
            "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
        "\n",
        "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
      ],
      "metadata": {
        "id": "PJ2vCQ_7C38K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # log out exp_values\n",
        "  print(\"exp_values\")\n",
        "  print(exp_values)\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # log out denom\n",
        "  print(\"denom\")\n",
        "  print(denom)\n",
        "  # Replicate denominator to N rows\n",
        "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
        "  # log out replicated denom\n",
        "  print(\"replicated denom\")\n",
        "  print(denom)\n",
        "  # Compute softmax\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax"
      ],
      "metadata": {
        "id": "obaQBdUAMXXv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Now let's compute self attention in matrix form\n",
        "def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "\n",
        "  # TODO -- Write this function\n",
        "  # 1. Compute queries, keys, and values\n",
        "  # 2. Compute dot products\n",
        "  # 3. Apply softmax to calculate attentions\n",
        "  # 4. Weight values by attentions\n",
        "  # Replace this line\n",
        "  XQ = omega_q @ X + beta_q\n",
        "  XK = omega_k @ X + beta_k\n",
        "  XV = omega_v @ X + beta_v\n",
        "  dots = np.matmul(XK.T,XQ)\n",
        "  attentions = softmax_cols(dots)\n",
        "  X_prime =  XV @ attentions\n",
        "  print(attentions)\n",
        "\n",
        "\n",
        "\n",
        "  return X_prime"
      ],
      "metadata": {
        "id": "gb2WvQ3SiH8r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy data into matrix\n",
        "X = np.zeros((D, N))\n",
        "X[:,0] = np.squeeze(all_x[0])\n",
        "X[:,1] = np.squeeze(all_x[1])\n",
        "X[:,2] = np.squeeze(all_x[2])\n",
        "\n",
        "# Run the self attention mechanism\n",
        "X_prime = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(X_prime)"
      ],
      "metadata": {
        "id": "MUOJbgJskUpl",
        "outputId": "60e5fb68-435c-4f59-a391-48826b44b7de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exp_values\n",
            "[[4.66985467e-14 2.21621546e-09 1.67867896e+00]\n",
            " [3.74967746e-01 4.64218522e+00 2.17350530e+00]\n",
            " [6.45495599e-04 7.88207480e+02 3.28094170e+02]]\n",
            "denom\n",
            "[3.75613241e-01 7.92849665e+02 3.31946354e+02]\n",
            "replicated denom\n",
            "[[3.75613241e-01 7.92849665e+02 3.31946354e+02]\n",
            " [3.75613241e-01 7.92849665e+02 3.31946354e+02]\n",
            " [3.75613241e-01 7.92849665e+02 3.31946354e+02]]\n",
            "[[1.24326146e-13 2.79525306e-12 5.05707907e-03]\n",
            " [9.98281489e-01 5.85506360e-03 6.54776072e-03]\n",
            " [1.71851130e-03 9.94144936e-01 9.88395160e-01]]\n",
            "[[ 0.94744244  1.64201168  1.61949281]\n",
            " [-0.24348429 -0.08470004 -0.06641533]\n",
            " [-0.91310441  4.02764044  3.96863308]\n",
            " [-0.44522983  2.18690791  2.15858316]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
        "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
        "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
        "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
        "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
        "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]"
      ],
      "metadata": {
        "id": "XgtNKttijlWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you did this correctly, the values should be the same as above.\n",
        "\n",
        "TODO:  \n",
        "\n",
        "Print out the attention matrix\n",
        "You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."
      ],
      "metadata": {
        "id": "as_lRKQFpvz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's compute self attention in matrix form\n",
        "def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "\n",
        "  # TODO -- Write this function\n",
        "  # 1. Compute queries, keys, and values\n",
        "  # 2. Compute dot products\n",
        "  # 3. Scale the dot products as in equation 12.9\n",
        "  # 4. Apply softmax to calculate attentions\n",
        "  # 5. Weight values by attentions\n",
        "  # Replace this line\n",
        "  X_prime = np.zeros_like(X);\n",
        "  XQ = omega_q @ X + beta_q\n",
        "  XK = omega_k @ X + beta_k\n",
        "  XV = omega_v @ X + beta_v\n",
        "  dots = np.matmul(XK.T,XQ)\n",
        "  attentions = softmax_cols(dots / np.sqrt(D))\n",
        "  # log out the attentions\n",
        "  print(\"attentions\")\n",
        "  print(attentions)\n",
        "\n",
        "  X_prime =  XV @ attentions\n",
        "\n",
        "  return X_prime"
      ],
      "metadata": {
        "id": "kLU7PUnnqvIh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the self attention mechanism\n",
        "X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "\n",
        "# Print out the results\n",
        "print(\"X_prime\")\n",
        "print(X_prime)"
      ],
      "metadata": {
        "id": "n18e3XNzmVgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8496afa-dbd3-483a-8f99-bcedce6da13e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exp_values\n",
            "[[2.16098465e-07 4.70766976e-05 1.29563844e+00]\n",
            " [6.12346100e-01 2.15457309e+00 1.47428128e+00]\n",
            " [2.54066054e-02 2.80750330e+01 1.81133699e+01]]\n",
            "denom\n",
            "[ 0.63775292 30.22965321 20.88328965]\n",
            "replicated denom\n",
            "[[ 0.63775292 30.22965321 20.88328965]\n",
            " [ 0.63775292 30.22965321 20.88328965]\n",
            " [ 0.63775292 30.22965321 20.88328965]]\n",
            "attentions\n",
            "[[3.38843552e-07 1.55730194e-06 6.20418746e-02]\n",
            " [9.60161968e-01 7.12734969e-02 7.05962187e-02]\n",
            " [3.98376935e-02 9.28724946e-01 8.67361907e-01]]\n",
            "X_prime\n",
            "[[ 0.97411966  1.59622051  1.32638014]\n",
            " [-0.23738409 -0.09516106  0.13062402]\n",
            " [-0.72333202  3.70194096  3.02371664]\n",
            " [-0.34413007  2.01339538  1.6902419 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO -- Investigate whether the self-attention mechanism is covariant with respect to permutation.\n",
        "If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted.\n"
      ],
      "metadata": {
        "id": "QDEkIrcgrql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "X7pcEsVgTXPz",
        "outputId": "3ac1b7d8-4363-4d0c-88a3-7edba238e93f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.78862847, -0.2773882 , -0.04381817],\n",
              "       [ 0.43650985, -0.35475898, -0.47721803],\n",
              "       [ 0.09649747, -0.08274148, -1.31386475],\n",
              "       [-1.8634927 , -0.62700068,  0.88462238]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let p1 be the identity, p2, p3, etc be all the other 3x3 permutation matrices\n",
        "import itertools\n",
        "\n",
        "def generate_permutation_matrices(n):\n",
        "  \"\"\"Generates all n x n permutation matrices.\n",
        "\n",
        "  Args:\n",
        "    n: The dimension of the square matrix.\n",
        "\n",
        "  Returns:\n",
        "    A list of n x n NumPy arrays, where each array is a permutation matrix.\n",
        "  \"\"\"\n",
        "  permutation_matrices = []\n",
        "  # Get all permutations of the indices [0, 1, ..., n-1]\n",
        "  for permutation in itertools.permutations(range(n)):\n",
        "    # Create an identity matrix of size n x n\n",
        "    matrix = np.eye(n)\n",
        "    # Permute the columns of the identity matrix according to the permutation\n",
        "    permuted_matrix = matrix[:, permutation]\n",
        "    permutation_matrices.append(permuted_matrix)\n",
        "  return permutation_matrices\n",
        "\n"
      ],
      "metadata": {
        "id": "HOd1tBtKTZSF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "permutation_matrices_3x3 = generate_permutation_matrices(3)\n"
      ],
      "metadata": {
        "id": "fK_7QkmpUJL3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "permutation_matrices_3x3"
      ],
      "metadata": {
        "id": "_3FmoLsPUJ8d",
        "outputId": "20b8b425-9ff5-4357-f27f-f4c6fe7fc92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.]]),\n",
              " array([[1., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.]]),\n",
              " array([[0., 1., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 0., 1.]]),\n",
              " array([[0., 0., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 1., 0.]]),\n",
              " array([[0., 1., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 0.]]),\n",
              " array([[0., 0., 1.],\n",
              "        [0., 1., 0.],\n",
              "        [1., 0., 0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "permuted_Xs = [np.matmul(X,permutation_matrices_3x3[i]) for i in range(len(permutation_matrices_3x3))]"
      ],
      "metadata": {
        "id": "IVtHAg3qUXTj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "permuted_Xs"
      ],
      "metadata": {
        "id": "gOdotm3RUvmX",
        "outputId": "6278ac03-e6ce-4c8a-989d-73dac88edf1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 1.78862847, -0.2773882 , -0.04381817],\n",
              "        [ 0.43650985, -0.35475898, -0.47721803],\n",
              "        [ 0.09649747, -0.08274148, -1.31386475],\n",
              "        [-1.8634927 , -0.62700068,  0.88462238]]),\n",
              " array([[ 1.78862847, -0.04381817, -0.2773882 ],\n",
              "        [ 0.43650985, -0.47721803, -0.35475898],\n",
              "        [ 0.09649747, -1.31386475, -0.08274148],\n",
              "        [-1.8634927 ,  0.88462238, -0.62700068]]),\n",
              " array([[-0.2773882 ,  1.78862847, -0.04381817],\n",
              "        [-0.35475898,  0.43650985, -0.47721803],\n",
              "        [-0.08274148,  0.09649747, -1.31386475],\n",
              "        [-0.62700068, -1.8634927 ,  0.88462238]]),\n",
              " array([[-0.2773882 , -0.04381817,  1.78862847],\n",
              "        [-0.35475898, -0.47721803,  0.43650985],\n",
              "        [-0.08274148, -1.31386475,  0.09649747],\n",
              "        [-0.62700068,  0.88462238, -1.8634927 ]]),\n",
              " array([[-0.04381817,  1.78862847, -0.2773882 ],\n",
              "        [-0.47721803,  0.43650985, -0.35475898],\n",
              "        [-1.31386475,  0.09649747, -0.08274148],\n",
              "        [ 0.88462238, -1.8634927 , -0.62700068]]),\n",
              " array([[-0.04381817, -0.2773882 ,  1.78862847],\n",
              "        [-0.47721803, -0.35475898,  0.43650985],\n",
              "        [-1.31386475, -0.08274148,  0.09649747],\n",
              "        [ 0.88462238, -0.62700068, -1.8634927 ]])]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[scaled_dot_product_self_attention(Y,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k) for Y in permuted_Xs]"
      ],
      "metadata": {
        "id": "C4DFwwDUUxJ1",
        "outputId": "d7fd464e-5a80-469f-dfc3-a8efd3a0f709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exp_values\n",
            "[[2.16098465e-07 4.70766976e-05 1.29563844e+00]\n",
            " [6.12346100e-01 2.15457309e+00 1.47428128e+00]\n",
            " [2.54066054e-02 2.80750330e+01 1.81133699e+01]]\n",
            "denom\n",
            "[ 0.63775292 30.22965321 20.88328965]\n",
            "replicated denom\n",
            "[[ 0.63775292 30.22965321 20.88328965]\n",
            " [ 0.63775292 30.22965321 20.88328965]\n",
            " [ 0.63775292 30.22965321 20.88328965]]\n",
            "attentions\n",
            "[[3.38843552e-07 1.55730194e-06 6.20418746e-02]\n",
            " [9.60161968e-01 7.12734969e-02 7.05962187e-02]\n",
            " [3.98376935e-02 9.28724946e-01 8.67361907e-01]]\n",
            "exp_values\n",
            "[[2.16098465e-07 1.29563844e+00 4.70766976e-05]\n",
            " [2.54066054e-02 1.81133699e+01 2.80750330e+01]\n",
            " [6.12346100e-01 1.47428128e+00 2.15457309e+00]]\n",
            "denom\n",
            "[ 0.63775292 20.88328965 30.22965321]\n",
            "replicated denom\n",
            "[[ 0.63775292 20.88328965 30.22965321]\n",
            " [ 0.63775292 20.88328965 30.22965321]\n",
            " [ 0.63775292 20.88328965 30.22965321]]\n",
            "attentions\n",
            "[[3.38843552e-07 6.20418746e-02 1.55730194e-06]\n",
            " [3.98376935e-02 8.67361907e-01 9.28724946e-01]\n",
            " [9.60161968e-01 7.05962187e-02 7.12734969e-02]]\n",
            "exp_values\n",
            "[[2.15457309e+00 6.12346100e-01 1.47428128e+00]\n",
            " [4.70766976e-05 2.16098465e-07 1.29563844e+00]\n",
            " [2.80750330e+01 2.54066054e-02 1.81133699e+01]]\n",
            "denom\n",
            "[30.22965321  0.63775292 20.88328965]\n",
            "replicated denom\n",
            "[[30.22965321  0.63775292 20.88328965]\n",
            " [30.22965321  0.63775292 20.88328965]\n",
            " [30.22965321  0.63775292 20.88328965]]\n",
            "attentions\n",
            "[[7.12734969e-02 9.60161968e-01 7.05962187e-02]\n",
            " [1.55730194e-06 3.38843552e-07 6.20418746e-02]\n",
            " [9.28724946e-01 3.98376935e-02 8.67361907e-01]]\n",
            "exp_values\n",
            "[[2.15457309e+00 1.47428128e+00 6.12346100e-01]\n",
            " [2.80750330e+01 1.81133699e+01 2.54066054e-02]\n",
            " [4.70766976e-05 1.29563844e+00 2.16098465e-07]]\n",
            "denom\n",
            "[30.22965321 20.88328965  0.63775292]\n",
            "replicated denom\n",
            "[[30.22965321 20.88328965  0.63775292]\n",
            " [30.22965321 20.88328965  0.63775292]\n",
            " [30.22965321 20.88328965  0.63775292]]\n",
            "attentions\n",
            "[[7.12734969e-02 7.05962187e-02 9.60161968e-01]\n",
            " [9.28724946e-01 8.67361907e-01 3.98376935e-02]\n",
            " [1.55730194e-06 6.20418746e-02 3.38843552e-07]]\n",
            "exp_values\n",
            "[[1.81133699e+01 2.54066054e-02 2.80750330e+01]\n",
            " [1.29563844e+00 2.16098465e-07 4.70766976e-05]\n",
            " [1.47428128e+00 6.12346100e-01 2.15457309e+00]]\n",
            "denom\n",
            "[20.88328965  0.63775292 30.22965321]\n",
            "replicated denom\n",
            "[[20.88328965  0.63775292 30.22965321]\n",
            " [20.88328965  0.63775292 30.22965321]\n",
            " [20.88328965  0.63775292 30.22965321]]\n",
            "attentions\n",
            "[[8.67361907e-01 3.98376935e-02 9.28724946e-01]\n",
            " [6.20418746e-02 3.38843552e-07 1.55730194e-06]\n",
            " [7.05962187e-02 9.60161968e-01 7.12734969e-02]]\n",
            "exp_values\n",
            "[[1.81133699e+01 2.80750330e+01 2.54066054e-02]\n",
            " [1.47428128e+00 2.15457309e+00 6.12346100e-01]\n",
            " [1.29563844e+00 4.70766976e-05 2.16098465e-07]]\n",
            "denom\n",
            "[20.88328965 30.22965321  0.63775292]\n",
            "replicated denom\n",
            "[[20.88328965 30.22965321  0.63775292]\n",
            " [20.88328965 30.22965321  0.63775292]\n",
            " [20.88328965 30.22965321  0.63775292]]\n",
            "attentions\n",
            "[[8.67361907e-01 9.28724946e-01 3.98376935e-02]\n",
            " [7.05962187e-02 7.12734969e-02 9.60161968e-01]\n",
            " [6.20418746e-02 1.55730194e-06 3.38843552e-07]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.97411966,  1.59622051,  1.32638014],\n",
              "        [-0.23738409, -0.09516106,  0.13062402],\n",
              "        [-0.72333202,  3.70194096,  3.02371664],\n",
              "        [-0.34413007,  2.01339538,  1.6902419 ]]),\n",
              " array([[ 0.97411966,  1.32638014,  1.59622051],\n",
              "        [-0.23738409,  0.13062402, -0.09516106],\n",
              "        [-0.72333202,  3.02371664,  3.70194096],\n",
              "        [-0.34413007,  1.6902419 ,  2.01339538]]),\n",
              " array([[ 1.59622051,  0.97411966,  1.32638014],\n",
              "        [-0.09516106, -0.23738409,  0.13062402],\n",
              "        [ 3.70194096, -0.72333202,  3.02371664],\n",
              "        [ 2.01339538, -0.34413007,  1.6902419 ]]),\n",
              " array([[ 1.59622051,  1.32638014,  0.97411966],\n",
              "        [-0.09516106,  0.13062402, -0.23738409],\n",
              "        [ 3.70194096,  3.02371664, -0.72333202],\n",
              "        [ 2.01339538,  1.6902419 , -0.34413007]]),\n",
              " array([[ 1.32638014,  0.97411966,  1.59622051],\n",
              "        [ 0.13062402, -0.23738409, -0.09516106],\n",
              "        [ 3.02371664, -0.72333202,  3.70194096],\n",
              "        [ 1.6902419 , -0.34413007,  2.01339538]]),\n",
              " array([[ 1.32638014,  1.59622051,  0.97411966],\n",
              "        [ 0.13062402, -0.09516106, -0.23738409],\n",
              "        [ 3.02371664,  3.70194096, -0.72333202],\n",
              "        [ 1.6902419 ,  2.01339538, -0.34413007]])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "permuted_outputs = [scaled_dot_product_self_attention(Y,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k) for Y in permuted_Xs]"
      ],
      "metadata": {
        "id": "CYeWu_N7VLwO",
        "outputId": "aba93573-fa10-4346-da23-2766b2c8f42f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exp_values\n",
            "[[2.16098465e-07 4.70766976e-05 1.29563844e+00]\n",
            " [6.12346100e-01 2.15457309e+00 1.47428128e+00]\n",
            " [2.54066054e-02 2.80750330e+01 1.81133699e+01]]\n",
            "denom\n",
            "[ 0.63775292 30.22965321 20.88328965]\n",
            "replicated denom\n",
            "[[ 0.63775292 30.22965321 20.88328965]\n",
            " [ 0.63775292 30.22965321 20.88328965]\n",
            " [ 0.63775292 30.22965321 20.88328965]]\n",
            "attentions\n",
            "[[3.38843552e-07 1.55730194e-06 6.20418746e-02]\n",
            " [9.60161968e-01 7.12734969e-02 7.05962187e-02]\n",
            " [3.98376935e-02 9.28724946e-01 8.67361907e-01]]\n",
            "exp_values\n",
            "[[2.16098465e-07 1.29563844e+00 4.70766976e-05]\n",
            " [2.54066054e-02 1.81133699e+01 2.80750330e+01]\n",
            " [6.12346100e-01 1.47428128e+00 2.15457309e+00]]\n",
            "denom\n",
            "[ 0.63775292 20.88328965 30.22965321]\n",
            "replicated denom\n",
            "[[ 0.63775292 20.88328965 30.22965321]\n",
            " [ 0.63775292 20.88328965 30.22965321]\n",
            " [ 0.63775292 20.88328965 30.22965321]]\n",
            "attentions\n",
            "[[3.38843552e-07 6.20418746e-02 1.55730194e-06]\n",
            " [3.98376935e-02 8.67361907e-01 9.28724946e-01]\n",
            " [9.60161968e-01 7.05962187e-02 7.12734969e-02]]\n",
            "exp_values\n",
            "[[2.15457309e+00 6.12346100e-01 1.47428128e+00]\n",
            " [4.70766976e-05 2.16098465e-07 1.29563844e+00]\n",
            " [2.80750330e+01 2.54066054e-02 1.81133699e+01]]\n",
            "denom\n",
            "[30.22965321  0.63775292 20.88328965]\n",
            "replicated denom\n",
            "[[30.22965321  0.63775292 20.88328965]\n",
            " [30.22965321  0.63775292 20.88328965]\n",
            " [30.22965321  0.63775292 20.88328965]]\n",
            "attentions\n",
            "[[7.12734969e-02 9.60161968e-01 7.05962187e-02]\n",
            " [1.55730194e-06 3.38843552e-07 6.20418746e-02]\n",
            " [9.28724946e-01 3.98376935e-02 8.67361907e-01]]\n",
            "exp_values\n",
            "[[2.15457309e+00 1.47428128e+00 6.12346100e-01]\n",
            " [2.80750330e+01 1.81133699e+01 2.54066054e-02]\n",
            " [4.70766976e-05 1.29563844e+00 2.16098465e-07]]\n",
            "denom\n",
            "[30.22965321 20.88328965  0.63775292]\n",
            "replicated denom\n",
            "[[30.22965321 20.88328965  0.63775292]\n",
            " [30.22965321 20.88328965  0.63775292]\n",
            " [30.22965321 20.88328965  0.63775292]]\n",
            "attentions\n",
            "[[7.12734969e-02 7.05962187e-02 9.60161968e-01]\n",
            " [9.28724946e-01 8.67361907e-01 3.98376935e-02]\n",
            " [1.55730194e-06 6.20418746e-02 3.38843552e-07]]\n",
            "exp_values\n",
            "[[1.81133699e+01 2.54066054e-02 2.80750330e+01]\n",
            " [1.29563844e+00 2.16098465e-07 4.70766976e-05]\n",
            " [1.47428128e+00 6.12346100e-01 2.15457309e+00]]\n",
            "denom\n",
            "[20.88328965  0.63775292 30.22965321]\n",
            "replicated denom\n",
            "[[20.88328965  0.63775292 30.22965321]\n",
            " [20.88328965  0.63775292 30.22965321]\n",
            " [20.88328965  0.63775292 30.22965321]]\n",
            "attentions\n",
            "[[8.67361907e-01 3.98376935e-02 9.28724946e-01]\n",
            " [6.20418746e-02 3.38843552e-07 1.55730194e-06]\n",
            " [7.05962187e-02 9.60161968e-01 7.12734969e-02]]\n",
            "exp_values\n",
            "[[1.81133699e+01 2.80750330e+01 2.54066054e-02]\n",
            " [1.47428128e+00 2.15457309e+00 6.12346100e-01]\n",
            " [1.29563844e+00 4.70766976e-05 2.16098465e-07]]\n",
            "denom\n",
            "[20.88328965 30.22965321  0.63775292]\n",
            "replicated denom\n",
            "[[20.88328965 30.22965321  0.63775292]\n",
            " [20.88328965 30.22965321  0.63775292]\n",
            " [20.88328965 30.22965321  0.63775292]]\n",
            "attentions\n",
            "[[8.67361907e-01 9.28724946e-01 3.98376935e-02]\n",
            " [7.05962187e-02 7.12734969e-02 9.60161968e-01]\n",
            " [6.20418746e-02 1.55730194e-06 3.38843552e-07]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "permuted_outputs"
      ],
      "metadata": {
        "id": "WdOHkjrHVcJO",
        "outputId": "efb14894-07ea-4516-cae3-066154e1d12b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.97411966,  1.59622051,  1.32638014],\n",
              "        [-0.23738409, -0.09516106,  0.13062402],\n",
              "        [-0.72333202,  3.70194096,  3.02371664],\n",
              "        [-0.34413007,  2.01339538,  1.6902419 ]]),\n",
              " array([[ 0.97411966,  1.32638014,  1.59622051],\n",
              "        [-0.23738409,  0.13062402, -0.09516106],\n",
              "        [-0.72333202,  3.02371664,  3.70194096],\n",
              "        [-0.34413007,  1.6902419 ,  2.01339538]]),\n",
              " array([[ 1.59622051,  0.97411966,  1.32638014],\n",
              "        [-0.09516106, -0.23738409,  0.13062402],\n",
              "        [ 3.70194096, -0.72333202,  3.02371664],\n",
              "        [ 2.01339538, -0.34413007,  1.6902419 ]]),\n",
              " array([[ 1.59622051,  1.32638014,  0.97411966],\n",
              "        [-0.09516106,  0.13062402, -0.23738409],\n",
              "        [ 3.70194096,  3.02371664, -0.72333202],\n",
              "        [ 2.01339538,  1.6902419 , -0.34413007]]),\n",
              " array([[ 1.32638014,  0.97411966,  1.59622051],\n",
              "        [ 0.13062402, -0.23738409, -0.09516106],\n",
              "        [ 3.02371664, -0.72333202,  3.70194096],\n",
              "        [ 1.6902419 , -0.34413007,  2.01339538]]),\n",
              " array([[ 1.32638014,  1.59622051,  0.97411966],\n",
              "        [ 0.13062402, -0.09516106, -0.23738409],\n",
              "        [ 3.02371664,  3.70194096, -0.72333202],\n",
              "        [ 1.6902419 ,  2.01339538, -0.34413007]])]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Determine if all of the elements of permuted_outputs have the same columns but possibly in different orders within a numerical tolerance of 0.0001\n",
        "\n",
        "import numpy as np\n",
        "def matrices_are_column_permutation_within_tolerance(matrix1, matrix2, tolerance=1e-4):\n",
        "  \"\"\"\n",
        "  Checks if two matrices have the same columns within a tolerance,\n",
        "  allowing for different column orders.\n",
        "\n",
        "  Args:\n",
        "    matrix1: The first NumPy array.\n",
        "    matrix2: The second NumPy array.\n",
        "    tolerance: The numerical tolerance for comparison.\n",
        "\n",
        "  Returns:\n",
        "    True if the matrices have the same columns within the tolerance, False otherwise.\n",
        "  \"\"\"\n",
        "  if matrix1.shape != matrix2.shape:\n",
        "    return False\n",
        "\n",
        "  # Create lists of columns for easier comparison\n",
        "  cols1 = [matrix1[:, i] for i in range(matrix1.shape[1])]\n",
        "  cols2 = [matrix2[:, i] for i in range(matrix2.shape[1])]\n",
        "\n",
        "  # Check if every column in cols1 is close to a column in cols2\n",
        "  for col1 in cols1:\n",
        "    found_match = False\n",
        "    for col2 in cols2:\n",
        "      if np.allclose(col1, col2, atol=tolerance):\n",
        "        found_match = True\n",
        "        break\n",
        "    if not found_match:\n",
        "      return False\n",
        "\n",
        "  # Check if every column in cols2 is close to a column in cols1 (redundant but robust)\n",
        "  for col2 in cols2:\n",
        "    found_match = False\n",
        "    for col1 in cols1:\n",
        "      if np.allclose(col2, col1, atol=tolerance):\n",
        "        found_match = True\n",
        "        break\n",
        "    if not found_match:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "# Check if all pairs of matrices in permuted_outputs have the same columns\n",
        "all_same_columns = True\n",
        "for i in range(len(permuted_outputs)):\n",
        "  for j in range(i + 1, len(permuted_outputs)):\n",
        "    if not matrices_are_column_permutation_within_tolerance(permuted_outputs[i], permuted_outputs[j], tolerance=0.0001):\n",
        "      all_same_columns = False\n",
        "      break\n",
        "  if not all_same_columns:\n",
        "    break\n",
        "\n",
        "print(f\"All permuted outputs have the same columns (possibly different order) within tolerance: {all_same_columns}\")\n",
        "\n",
        "# You can also check a specific pair\n",
        "# print(f\"Are the first two outputs column permutations: {matrices_are_column_permutation_within_tolerance(permuted_outputs[0], permuted_outputs[1], tolerance=0.0001)}\")\n"
      ],
      "metadata": {
        "id": "WNz79pskVexX",
        "outputId": "08ba608d-052f-4f94-b01b-7b8deb7c253f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All permuted outputs have the same columns (possibly different order) within tolerance: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: For each pair of elements of permuted_outputs, print the difference of the closest (in L1 norm) pair of columns.\n",
        "\n",
        "import numpy as np\n",
        "def find_closest_columns_difference(matrix1, matrix2):\n",
        "  \"\"\"\n",
        "  Finds the L1 norm difference between the closest pair of columns\n",
        "  between two matrices.\n",
        "\n",
        "  Args:\n",
        "    matrix1: The first NumPy array.\n",
        "    matrix2: The second NumPy array.\n",
        "\n",
        "  Returns:\n",
        "    The minimum L1 norm difference between any column in matrix1 and\n",
        "    any column in matrix2.\n",
        "  \"\"\"\n",
        "  min_diff = float('inf')\n",
        "  for col1 in matrix1.T:  # Iterate over columns of the first matrix\n",
        "    for col2 in matrix2.T:  # Iterate over columns of the second matrix\n",
        "      diff = np.sum(np.abs(col1 - col2))  # Calculate L1 norm difference\n",
        "      if diff < min_diff:\n",
        "        min_diff = diff\n",
        "  return min_diff\n",
        "\n",
        "for i in range(len(permuted_outputs)):\n",
        "    for j in range(i + 1, len(permuted_outputs)):\n",
        "        diff = find_closest_columns_difference(permuted_outputs[i], permuted_outputs[j])\n",
        "        print(f\"Difference of closest columns between matrix {i} and matrix {j}: {diff}\")\n"
      ],
      "metadata": {
        "id": "4IVygigCWYa4",
        "outputId": "cd00d37b-7856-48c2-941d-8fdeee62ff2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference of closest columns between matrix 0 and matrix 1: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 0 and matrix 2: 0.0\n",
            "Difference of closest columns between matrix 0 and matrix 3: 0.0\n",
            "Difference of closest columns between matrix 0 and matrix 4: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 0 and matrix 5: 2.220446049250313e-16\n",
            "Difference of closest columns between matrix 1 and matrix 2: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 1 and matrix 3: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 1 and matrix 4: 0.0\n",
            "Difference of closest columns between matrix 1 and matrix 5: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 2 and matrix 3: 0.0\n",
            "Difference of closest columns between matrix 2 and matrix 4: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 2 and matrix 5: 0.0\n",
            "Difference of closest columns between matrix 3 and matrix 4: 1.942890293094024e-16\n",
            "Difference of closest columns between matrix 3 and matrix 5: 0.0\n",
            "Difference of closest columns between matrix 4 and matrix 5: 0.0\n"
          ]
        }
      ]
    }
  ]
}